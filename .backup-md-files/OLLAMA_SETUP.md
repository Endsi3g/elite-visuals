# ü¶ô Configuration Ollama - LLM Open Source

Elite Visuals utilise maintenant **Ollama** pour les LLM open source au lieu d'OpenAI.

## üéØ Avantages

- ‚úÖ **100% Open Source** : Llama 3, Mistral, CodeLlama, etc.
- ‚úÖ **Gratuit** : Pas de co√ªts API
- ‚úÖ **Local** : Donn√©es priv√©es, pas d'envoi externe
- ‚úÖ **Rapide** : Ex√©cution locale sur GPU/CPU
- ‚úÖ **Fallback HuggingFace** : Si Ollama indisponible

## üì¶ Installation Ollama

### Windows

1. **T√©l√©charger Ollama**
   ```powershell
   # T√©l√©charger depuis https://ollama.ai/download
   # Ou via winget
   winget install Ollama.Ollama
   ```

2. **V√©rifier l'installation**
   ```powershell
   ollama --version
   ```

3. **D√©marrer le service**
   ```powershell
   # Ollama d√©marre automatiquement en arri√®re-plan
   # Par d√©faut sur http://localhost:11434
   ```

### macOS

```bash
# Via Homebrew
brew install ollama

# D√©marrer le service
ollama serve
```

### Linux

```bash
# Installation
curl -fsSL https://ollama.ai/install.sh | sh

# D√©marrer le service
ollama serve
```

## ü§ñ Mod√®les Disponibles

### Mod√®les Recommand√©s pour Elite Visuals

#### 1. **Llama 3** (Recommand√©)
```bash
ollama pull llama3
```
- **Taille** : 4.7 GB
- **Usage** : G√©n√©ration de texte g√©n√©ral, scripts, briefs
- **Performance** : Excellent en fran√ßais

#### 2. **Mistral**
```bash
ollama pull mistral
```
- **Taille** : 4.1 GB
- **Usage** : Cr√©ativit√©, storytelling
- **Performance** : Tr√®s bon en fran√ßais

#### 3. **LLaVA** (Vision)
```bash
ollama pull llava
```
- **Taille** : 4.7 GB
- **Usage** : Analyse d'images
- **Performance** : Multimodal (texte + image)

#### 4. **CodeLlama**
```bash
ollama pull codellama
```
- **Taille** : 3.8 GB
- **Usage** : G√©n√©ration de code
- **Performance** : Sp√©cialis√© code

### Autres Mod√®les

```bash
# Mod√®les plus petits (plus rapides)
ollama pull llama3:8b      # Version 8B param√®tres
ollama pull mistral:7b     # Version 7B param√®tres

# Mod√®les plus grands (meilleure qualit√©)
ollama pull llama3:70b     # Version 70B param√®tres (n√©cessite 40GB+ RAM)
ollama pull mixtral:8x7b   # Mixture of Experts
```

## ‚öôÔ∏è Configuration Elite Visuals

### Variables d'Environnement

√âditer `.env.local` :

```env
# Ollama Local
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3

# HuggingFace (Fallback optionnel)
HUGGINGFACE_API_KEY=hf_your_api_key
HF_MODEL=mistralai/Mistral-7B-Instruct-v0.2
```

### Changer de Mod√®le

```env
# Pour Mistral
OLLAMA_MODEL=mistral

# Pour LLaVA (analyse d'images)
OLLAMA_MODEL=llava

# Pour CodeLlama
OLLAMA_MODEL=codellama
```

## üöÄ Utilisation

### D√©marrer Ollama

```bash
# Le service d√©marre automatiquement
# Ou manuellement :
ollama serve
```

### Tester Ollama

```bash
# Test simple
ollama run llama3 "Bonjour, √©cris un script publicitaire de 30 secondes"

# Test avec LLaVA (vision)
ollama run llava "Analyse cette image"
```

### V√©rifier le Service

```bash
# V√©rifier que le service tourne
curl http://localhost:11434/api/tags
```

## üîÑ Fallback HuggingFace

Si Ollama n'est pas disponible, Elite Visuals utilise automatiquement HuggingFace Inference API.

### Obtenir une Cl√© HuggingFace

1. Cr√©er un compte sur [HuggingFace](https://huggingface.co/)
2. Aller dans Settings > Access Tokens
3. Cr√©er un nouveau token (Read access)
4. Ajouter dans `.env.local` :
   ```env
   HUGGINGFACE_API_KEY=hf_your_token_here
   ```

### Mod√®les HuggingFace Recommand√©s

```env
# Mistral 7B (Recommand√©)
HF_MODEL=mistralai/Mistral-7B-Instruct-v0.2

# Llama 2
HF_MODEL=meta-llama/Llama-2-7b-chat-hf

# Zephyr
HF_MODEL=HuggingFaceH4/zephyr-7b-beta
```

## üìä Comparaison des Options

| Option | Co√ªt | Vitesse | Qualit√© | Priv√© | Setup |
|--------|------|---------|---------|-------|-------|
| **Ollama Local** | Gratuit | Rapide | Tr√®s bon | ‚úÖ | Moyen |
| **HuggingFace API** | Gratuit* | Moyen | Bon | ‚ö†Ô∏è | Facile |
| **OpenAI (ancien)** | Payant | Rapide | Excellent | ‚ùå | Facile |

*HuggingFace gratuit avec rate limits

## üõ†Ô∏è D√©pannage

### Ollama ne d√©marre pas

```bash
# V√©rifier le statut
ollama list

# Red√©marrer le service
# Windows : Red√©marrer l'application Ollama
# macOS/Linux :
pkill ollama
ollama serve
```

### Port d√©j√† utilis√©

```bash
# Changer le port
OLLAMA_HOST=0.0.0.0:11435 ollama serve

# Mettre √† jour .env.local
OLLAMA_BASE_URL=http://localhost:11435
```

### Mod√®le trop lent

```bash
# Utiliser un mod√®le plus petit
ollama pull llama3:8b
# Puis dans .env.local
OLLAMA_MODEL=llama3:8b
```

### Erreur de m√©moire

```bash
# Mod√®les 7B-13B : 8GB RAM minimum
# Mod√®les 70B+ : 40GB+ RAM minimum

# Solution : Utiliser un mod√®le plus petit ou HuggingFace
```

## üé® Fonctionnalit√©s Elite Visuals

### G√©n√©ration de Scripts

```typescript
import { generateScript } from "@/lib/ai/ollama"

const result = await generateScript(
  "Cr√©e un script de 30 secondes pour une pub de parfum"
)
```

### Analyse d'Images

```typescript
import { analyzeMedia } from "@/lib/ai/ollama"

const result = await analyzeMedia("image", imageBase64)
```

### G√©n√©ration de Briefs

```typescript
import { generateBrief } from "@/lib/ai/ollama"

const result = await generateBrief([
  "Campagne pour une marque de luxe",
  "Cible : 25-45 ans",
  "Budget : 50k‚Ç¨"
])
```

## üìö Ressources

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Mod√®les Ollama](https://ollama.ai/library)
- [HuggingFace Models](https://huggingface.co/models)
- [Llama 3 Guide](https://ai.meta.com/llama/)

## üîê S√©curit√© & Confidentialit√©

### Avantages Ollama Local

- ‚úÖ Donn√©es restent sur votre machine
- ‚úÖ Pas d'envoi vers des serveurs externes
- ‚úÖ Conforme RGPD
- ‚úÖ Pas de logs externes

### HuggingFace API

- ‚ö†Ô∏è Donn√©es envoy√©es √† HuggingFace
- ‚ö†Ô∏è Soumis aux conditions d'utilisation HF
- ‚úÖ Chiffrement HTTPS
- ‚úÖ Pas de stockage permanent (selon mod√®le)

## üéØ Recommandations

### Pour D√©veloppement

```env
OLLAMA_MODEL=llama3:8b  # Plus rapide
```

### Pour Production

```env
OLLAMA_MODEL=llama3     # Meilleure qualit√©
# + HuggingFace en fallback
HUGGINGFACE_API_KEY=hf_xxx
```

### Pour Analyse d'Images

```env
OLLAMA_MODEL=llava
```

---

**Migration r√©ussie d'OpenAI vers Ollama** ‚úÖ  
**100% Open Source** ü¶ô  
**Co√ªt : 0‚Ç¨** üí∞
